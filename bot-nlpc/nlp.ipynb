{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\"\"\"Natural Language Processing (NLP)  is the ability of a computer program to understand human language.\n",
    "Here the following text mining operations have been shown using NLTK package in Python:\n",
    "\n",
    "(1) Tokenizing\n",
    "(2) Stop Words\n",
    "(3) Stemming\n",
    "(4) Part of Speech (POS) tagging\n",
    "(5) Chunking\n",
    "(6) Chinking\n",
    "(7) Named Entity Recognition\n",
    "(8) Lemmatizing\n",
    "(9) Wordnet \"\"\"\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# 跳过证书验证\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Bob, how are you doing today?', 'why are you playing outside?', 'Hope you come back soon.', 'Icecream smells good.']\n",
      "['Hello', 'Mr.', 'Bob', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'why', 'are', 'you', 'playing', 'outside', '?', 'Hope', 'you', 'come', 'back', 'soon', '.', 'Icecream', 'smells', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" (1) Tokenizing words and sentences (Lexical Analysis)\"\"\"\n",
    "\n",
    "\"\"\"Tokenization is a process of breaking up a given text into words or sentences. Eg: Word Tokenizer and Sentence Tokenizer.\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sample_text = \"Hello Mr. Bob, how are you doing today? why are you playing outside? Hope you come back soon. Icecream smells good.\"\n",
    "print(sent_tokenize(sample_text))\n",
    "print(word_tokenize(sample_text))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" (2) Stop words are the most commonly used words which are removed/ignored in Natural Language Processing\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sample_text = \"This is just an example to show stop words removals.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(filtered_sentence)\n",
    "filtered_sentence = [ w for w in words if not w in stop_words]\n",
    "print(filtered_sentence)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" (3) Stemming is a process of reducing words to its root form even if the root has no dictionary meaning.  \"\"\"\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sample_text = [\"python\", \"pythoner\", \"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for w in sample_text:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" (4) POS Tagging is assigning part of speech tags to words\"\"\"\n",
    "\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "\n",
    "\"\"\" PunktSentenceTokenizer is an sentence boundary detection algorithm that must be trained to be used. NLTK includes a pre-trained version of the PunktSentenceTokenizer. \"\"\"\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(tokenized)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "print(process_content())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" (5) Chunking is a process of grouping words into chunks. These are phrases of one or more words that contain noun. \"\"\"\n",
    "\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "print(process_content())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Chinking is a process of removing a chunk from a chunk. This chunk which needs to be removed is called chink \"\"\"\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "\"\"\" PunktSentenceTokenizer is an sentence boundary detection algorithm that must be trained to be used. NLTK includes a pre-trained version of the PunktSentenceTokenizer. \"\"\"\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "print(process_content())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Named Entity Recognition is a process to extract \"entities\" like people, place, locations, monetary figures and more \"\"\"\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\"\"\" PunktSentenceTokenizer is an sentence boundary detection algorithm that must be trained to be used. NLTK includes a pre-trained version of the PunktSentenceTokenizer. \"\"\"\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "print(process_content())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Lemmatisation is a process of reducing words into their lemma or dictionary. It takes into account the meaning of the word in the sentence.\"\"\"\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" WordNet is a lexical database of English language, which was created by Princeton, and is part of the NLTK corpus.\n",
    "It can be used to find meanings of words, synonyms, antonyms and more.\"\"\"\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"program\")\n",
    "\n",
    "print(syns)\n",
    "print(syns[0])\n",
    "print(syns[0].name())\n",
    "print(syns[0].lemmas())\n",
    "print(syns[0].lemmas()[0].name)\n",
    "print(syns[0].definition())\n",
    "print(syns[0].examples())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}